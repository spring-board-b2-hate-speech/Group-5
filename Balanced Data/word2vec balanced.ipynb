{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1722a68c-a259-4963-980f-4525130bb103",
   "metadata": {},
   "source": [
    "# Imports and Loading SpaCy Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d346925-a3a5-4681-be84-36539ec26a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5484e2-ce94-4695-b7d3-988b058fbf92",
   "metadata": {},
   "source": [
    "# Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726a808c-ae26-46ec-9de0-d98f3f815b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Arabs are not welcome</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I'm not saying we should actually eliminate he...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41139</th>\n",
       "      <td>117100</td>\n",
       "      <td>Indian culture is so backwards</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41140</th>\n",
       "      <td>118100</td>\n",
       "      <td>Women don't want equality, they just want to b...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41141</th>\n",
       "      <td>119100</td>\n",
       "      <td>fuck covid</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41142</th>\n",
       "      <td>1205</td>\n",
       "      <td>This computer is garbage</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41143</th>\n",
       "      <td>121100</td>\n",
       "      <td>The only good Muslim is a dead one</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41144 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                            comment label\n",
       "0               1                                dalits are lowlives     N\n",
       "1               2             gay people are a burden to our society     N\n",
       "2               3                              Arabs are not welcome     N\n",
       "3               4  I'm not saying we should actually eliminate he...     N\n",
       "4               5                       bananas are for black people     N\n",
       "...           ...                                                ...   ...\n",
       "41139      117100                     Indian culture is so backwards     N\n",
       "41140      118100  Women don't want equality, they just want to b...     N\n",
       "41141      119100                                         fuck covid     P\n",
       "41142        1205                           This computer is garbage     P\n",
       "41143      121100                 The only good Muslim is a dead one     N\n",
       "\n",
       "[41144 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('hatspeech dataset.csv', encoding='ISO-8859-1')  \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb212a7-27b5-4475-90ad-3506d4b08a47",
   "metadata": {},
   "source": [
    "# Data Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9057178-16b4-4299-bccb-2952731db77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                            comment label\n",
      "0           1                                dalits are lowlives     N\n",
      "1           2             gay people are a burden to our society     N\n",
      "2           3                              Arabs are not welcome     N\n",
      "3           4  I'm not saying we should actually eliminate he...     N\n",
      "4           5                       bananas are for black people     N\n"
     ]
    }
   ],
   "source": [
    "print(data.head())  # Check the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79cb4f8-5c08-4260-b1df-def58def6261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41144 entries, 0 to 41143\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  41144 non-null  int64 \n",
      " 1   comment     41144 non-null  object\n",
      " 2   label       41144 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 964.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())  # Get information about the dataset, like column names and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f00efe92-f98b-41cc-b95f-450fcf0532ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "N    22158\n",
      "P    18950\n",
      "O       36\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['label'].value_counts())  # Check the distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8b3daa3-3d37-4e10-a02f-4515b9056506",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['label'] != 'O'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a44669-7b60-4206-9264-dad8a96430bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "N    22158\n",
      "P    18950\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9e30765-c9db-4d21-9e17-c8ce1513dc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    False\n",
       "comment       False\n",
       "label         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cff9423-1e05-4067-acfd-d1f9b109869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "#Find the number of duplicate rows\n",
    "num_duplicates = data.duplicated().sum()\n",
    "print(f'Number of duplicate rows: {num_duplicates}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c76a4df-d7a9-4c67-9941-019a964d08d9",
   "metadata": {},
   "source": [
    "# Text Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15adf2b-0c10-4d27-9ea8-5207ab4e16a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maddi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maddi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maddi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d4bf830-65ae-46a5-8213-ed14083d467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatword_dictionary = {\n",
    "    'u': 'you',\n",
    "    'ur': 'your',\n",
    "    'r': 'are',\n",
    "    'y': 'why',\n",
    "    'b4': 'before',\n",
    "    'gr8': 'great',\n",
    "    'l8r': 'later',\n",
    "    'w8': 'wait',\n",
    "    'bff': 'best friend forever',\n",
    "    'brb': 'be right back',\n",
    "    'btw': 'by the way',\n",
    "    'cuz': 'because',\n",
    "    'idk': 'i do not know',\n",
    "    'ikr': 'i know right',\n",
    "    'imo': 'in my opinion',\n",
    "    'lmao': 'laughing my ass off',\n",
    "    'lol': 'laugh out loud',\n",
    "    'omg': 'oh my god',\n",
    "    'omw': 'on my way',\n",
    "    'pls': 'please',\n",
    "    'thx': 'thanks',\n",
    "    'ttyl': 'talk to you later',\n",
    "    'wth': 'what the hell',\n",
    "    'wyd': 'what you doing',\n",
    "    'smh': 'shaking my head'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98b861c7-31e6-44d6-a7a6-d78701f70081",
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {\n",
    "    \"kinda\": \"kind of\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"idk\": \"I do not know\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"bbl\": \"be back later\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7acd7001-1019-429a-bfd6-9c7613788fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  text = text.lower()  # Convert text to lowercase\n",
    "  text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "  for chatword, full_form in chatword_dictionary.items():\n",
    "      text = re.sub(rf\"\\b{chatword}\\b\", full_form, text)\n",
    "  for slang, expanded in slang_dict.items():\n",
    "      text = re.sub(r'\\b' + re.escape(slang) + r'\\b', expanded, text)\n",
    "  text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
    "  text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "  text = re.sub(r'(.)\\1{2,}', r'\\1', text)  # Replace sequences of the same character > 2 with 1 occurrence\n",
    "  text = re.sub(r'(\\w)[^\\w\\s](\\w)', r'\\1\\2', text)  # Remove special characters within words like 'b@ckward' to 'backward'\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a65f10fb-47c2-440e-841c-733cfd76288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['comment'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ed82282-cef7-47f2-a1f0-0fe59e2e957e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                      dalits are lowlives\n",
      "1                   gay people are a burden to our society\n",
      "2                                    arabs are not welcome\n",
      "3        i am not saying we should actually eliminate h...\n",
      "4                             bananas are for black people\n",
      "                               ...                        \n",
      "41139                       indian culture is so backwards\n",
      "41140    women do not want equality they just want to b...\n",
      "41141                                           fuck covid\n",
      "41142                             this computer is garbage\n",
      "41143                   the only good muslim is a dead one\n",
      "Name: clean_text, Length: 41108, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abb7fd-2482-4a53-92c9-506422c8a639",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c67eb69c-1004-4de5-8cd2-81b641c452b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  [dalits, are, lowlives]\n",
       "1          [gay, people, are, a, burden, to, our, society]\n",
       "2                               [arabs, are, not, welcome]\n",
       "3        [i, am, not, saying, we, should, actually, eli...\n",
       "4                       [bananas, are, for, black, people]\n",
       "                               ...                        \n",
       "41139                 [indian, culture, is, so, backwards]\n",
       "41140    [women, do, not, want, equality, they, just, w...\n",
       "41141                                        [fuck, covid]\n",
       "41142                        [this, computer, is, garbage]\n",
       "41143          [the, only, good, muslim, is, a, dead, one]\n",
       "Name: tokens, Length: 41108, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data['tokens'] = data['clean_text'].apply(word_tokenize)\n",
    "data['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c35abe-9047-4e41-b20e-8f86a8e2af87",
   "metadata": {},
   "source": [
    "# Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a938c18-1caa-4b03-925f-a39ab1f0e856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       [dalits, lowlives]\n",
       "1                           [gay, people, burden, society]\n",
       "2                                         [arabs, welcome]\n",
       "3        [saying, actually, eliminate, heebs, wish, nat...\n",
       "4                                 [bananas, black, people]\n",
       "                               ...                        \n",
       "41139                         [indian, culture, backwards]\n",
       "41140                [women, want, equality, want, charge]\n",
       "41141                                        [fuck, covid]\n",
       "41142                                  [computer, garbage]\n",
       "41143                            [good, muslim, dead, one]\n",
       "Name: remove_stopwords, Length: 41108, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(tokens):\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "data['remove_stopwords'] = data['tokens'].apply(remove_stop_words)\n",
    "data['remove_stopwords']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1bc5c6-8160-4b16-b5f3-f9027c62c208",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18587bbe-a6f0-4497-bac1-710d711a8ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          [dalit, lowliv]\n",
       "1                            [gay, peopl, burden, societi]\n",
       "2                                           [arab, welcom]\n",
       "3        [say, actual, elimin, heeb, wish, natur, becam...\n",
       "4                                   [banana, black, peopl]\n",
       "                               ...                        \n",
       "41139                           [indian, cultur, backward]\n",
       "41140                    [women, want, equal, want, charg]\n",
       "41141                                        [fuck, covid]\n",
       "41142                                     [comput, garbag]\n",
       "41143                            [good, muslim, dead, one]\n",
       "Name: stemmed, Length: 41108, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming\n",
    "ps= PorterStemmer()\n",
    "# Function to perform stemming\n",
    "def perform_stemming(tokens):\n",
    "    return [ps.stem(word) for word in tokens]\n",
    "data['stemmed'] = data['remove_stopwords'].apply(perform_stemming)\n",
    "data['stemmed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256bff8f-7d60-4d26-95c7-ff0b44721fb5",
   "metadata": {},
   "source": [
    "# Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24a3ca2e-c28f-4582-8223-7f1abe7dde35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       [dalits, lowlives]\n",
       "1                           [gay, people, burden, society]\n",
       "2                                          [arab, welcome]\n",
       "3        [saying, actually, eliminate, heebs, wish, nat...\n",
       "4                                  [banana, black, people]\n",
       "                               ...                        \n",
       "41139                         [indian, culture, backwards]\n",
       "41140                [woman, want, equality, want, charge]\n",
       "41141                                        [fuck, covid]\n",
       "41142                                  [computer, garbage]\n",
       "41143                            [good, muslim, dead, one]\n",
       "Name: lemmatized, Length: 41108, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# Function to perform lemmatization\n",
    "def perform_lemmatization(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "data['lemmatized'] = data['remove_stopwords'].apply(perform_lemmatization)\n",
    "data['lemmatized']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb60ca6-dfdc-4339-9b4d-155bab8d52ce",
   "metadata": {},
   "source": [
    "# Load spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8a61cd6-b6b1-43d5-8292-ea9a8c288b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49eb328-549f-438a-a6db-956cbe769704",
   "metadata": {},
   "source": [
    "# Function to get word2vec embeddings using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f503d597-015b-4805-a099-7f12eafe7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get word2vec embeddings using SpaCy\n",
    "def get_word2vec_embeddings(text_series):\n",
    "    embeddings = []\n",
    "    for doc in nlp.pipe(text_series, disable=[\"parser\", \"ner\"]):\n",
    "        if doc.has_vector:\n",
    "            embeddings.append(doc.vector)\n",
    "        else:\n",
    "            embeddings.append(None)  # In case the doc has no vectors\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e93571-3cef-4f26-a9a9-cad0dfe8641d",
   "metadata": {},
   "source": [
    "# Apply get_word2vec_embeddings to lemmatized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d5b5997-bbbb-49fd-8314-82e3469509d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply get_word2vec_embeddings to lemmatized text\n",
    "data['word2vec'] = get_word2vec_embeddings(data['lemmatized'].apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0eda437-2719-4aad-bc2e-f44dcaba8866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [-1.906, 1.2487, 1.9823, 1.9465, 1.79555, 1.50...\n",
       "1        [0.19075249, 2.0418324, -3.4573777, 0.86299753...\n",
       "2        [1.9986349, -5.041985, -0.76663, -1.51065, 1.4...\n",
       "3        [-1.7351625, -0.36887124, -0.23621875, -1.3883...\n",
       "4        [-2.5482068, 0.2703433, -3.9575834, 2.7313, 1....\n",
       "                               ...                        \n",
       "41139    [-0.96657497, -0.061333258, -0.79939, -0.07666...\n",
       "41140    [0.40465587, 2.970246, -3.5896618, -1.3439579,...\n",
       "41141    [3.7351, 1.0318251, -0.8066, 2.11059, -1.93250...\n",
       "41142    [0.08542502, -0.652035, 1.139975, 0.37980497, ...\n",
       "41143    [-2.26612, 1.6801, -1.407895, -0.5243, 3.7953,...\n",
       "Name: word2vec, Length: 41108, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['word2vec']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36b7a2-53af-48b8-9d90-06e255e817cf",
   "metadata": {},
   "source": [
    "# Balance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e34341e-59a2-4404-b428-7ac1960d22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the data\n",
    "X = np.vstack(data['word2vec'].dropna().values)\n",
    "y = data.loc[data['word2vec'].notna(), 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c45d7-83ad-40ed-845a-b84c87f41702",
   "metadata": {},
   "source": [
    "# Using RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5828e430-796a-4dc8-aba6-1632e43e9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81405428-55ed-4032-b2e4-163c036aef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21dd5ce4-77b8-414b-8693-3621c20f857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to DataFrame\n",
    "balanced_data = pd.DataFrame(X_res)\n",
    "balanced_data['label'] = y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51cc858c-e743-429e-8cf3-e7c7408defe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset label distribution:\n",
      "label\n",
      "N    18943\n",
      "P    18943\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Balanced dataset label distribution:\")\n",
    "print(balanced_data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d652701-3a20-460e-a13a-750e875a206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save balanced data to a new CSV file\n",
    "balanced_data.to_csv('balanced_hatespeech_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9d141-cb8c-4ac7-a68c-c1e65266c0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
