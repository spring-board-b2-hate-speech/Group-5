{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0271b294-94b5-436c-aed3-fd99037576b9",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b0837bf-d165-4d89-b734-536ddf587bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3290e8fb-f8bb-4d2c-b929-fe1ed271101e",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cfd2ca6-59f4-4003-b655-a5ff9d09fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Downloads/hate.csv (1)/hate.csv', encoding='latin1')  \n",
    "data = data.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54133a27-1cdc-43b3-a6f8-44aa0aa331a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabs are not welcome</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm not saying we should actually eliminate he...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41139</th>\n",
       "      <td>Indian culture is so backwards</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41140</th>\n",
       "      <td>Women don't want equality, they just want to b...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41141</th>\n",
       "      <td>fuck covid</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41142</th>\n",
       "      <td>This computer is garbage</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41143</th>\n",
       "      <td>The only good Muslim is a dead one</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41144 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment label\n",
       "0                                    dalits are lowlives     N\n",
       "1                 gay people are a burden to our society     N\n",
       "2                                  Arabs are not welcome     N\n",
       "3      I'm not saying we should actually eliminate he...     N\n",
       "4                           bananas are for black people     N\n",
       "...                                                  ...   ...\n",
       "41139                     Indian culture is so backwards     N\n",
       "41140  Women don't want equality, they just want to b...     N\n",
       "41141                                         fuck covid     P\n",
       "41142                           This computer is garbage     P\n",
       "41143                 The only good Muslim is a dead one     N\n",
       "\n",
       "[41144 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e53e11-21a0-4671-b42d-70bf3afe2cb0",
   "metadata": {},
   "source": [
    "# Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c978ab79-b288-49f3-9c6e-1449ee4e13a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment label\n",
      "0                                dalits are lowlives     N\n",
      "1             gay people are a burden to our society     N\n",
      "2                              Arabs are not welcome     N\n",
      "3  I'm not saying we should actually eliminate he...     N\n",
      "4                       bananas are for black people     N\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "354e5ae9-4615-4532-b9b7-ce595141c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41144 entries, 0 to 41143\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   comment  41144 non-null  object\n",
      " 1   label    41144 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 643.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd56b27b-9020-40e0-a0c6-48350b4e09ab",
   "metadata": {},
   "source": [
    "# Checking Distribution of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92ec500b-7309-4717-ba83-eb95a3852c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "N    22158\n",
      "P    18950\n",
      "O       36\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['label'].value_counts())  # Check the distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "815c4834-ce67-47db-8f86-5ce19957d403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment    False\n",
       "label      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1aad4d-d6fa-42d9-be2c-d3adaeb6fddf",
   "metadata": {},
   "source": [
    "# checking for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "065237d0-0a21-4c40-9d36-92bfb15a2bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 10\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "duplicates = data.duplicated().sum()\n",
    "print(\"\\nNumber of duplicate rows:\", duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42d41bba-197c-4ec5-8e60-b098cc021dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of duplicate rows:\n",
      "\n",
      "                        comment label\n",
      "41047  Blacks are such pondlife     N\n",
      "41048  Blacks are such pondlife     N\n",
      "41051  Blacks are such pondlife     N\n",
      "41055  Blacks are such pondlife     N\n",
      "41094    Muslims are a disgrace     N\n"
     ]
    }
   ],
   "source": [
    "if duplicates > 0:\n",
    "    print(\"\\nSample of duplicate rows:\\n\")\n",
    "    print(data[data.duplicated()].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29d3bd15-972e-4961-ba2e-8240d08e314a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicates removed. Number of rows now: 41134\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "data = data.drop_duplicates()\n",
    "print(\"\\nDuplicates removed. Number of rows now:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136eee8-e275-4a36-9ded-f03640b9c453",
   "metadata": {},
   "source": [
    "# Importing Necessary nltk Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8ec3b8f-7fc5-44fe-8086-b643d48cf1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ff987-4568-4b17-8851-9fcba82e0160",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50a02c-c4fe-4407-b5ac-c190eacc6a60",
   "metadata": {},
   "source": [
    "#### The `clean_text` function standardizes and cleans textual data by converting all characters to lowercase, removing non-word characters using a regular expression to replace them with spaces, and eliminating extra whitespace. This preprocessing step ensures the text is uniform and free of unwanted characters, making it more suitable for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0ae969d-18f2-45c0-a980-0a370f078569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Comprehensive dictionary of common contractions and chat language words\n",
    "contractions_dict = {\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"d'you\": \"do you\",\n",
    "    \"d'ya\": \"do you\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"n't\": \" not\",\n",
    "    \"'re\": \" are\",\n",
    "    \"'s\": \" is\",\n",
    "    \"'d\": \" would\",\n",
    "    \"'ll\": \" will\",\n",
    "    \"'t\": \" not\",\n",
    "    \"'ve\": \" have\",\n",
    "    \"'m\": \" am\",\n",
    "    \"y\": \"why\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"nah\": \"no\",\n",
    "    \"n\": \"and\",\n",
    "    \"whats'upp\": \"what is up\",\n",
    "    \"im\": \"i am\",\n",
    "    \"he'd\":\"he would\",\n",
    "    \"WMAF\":\"White Male Asian Female\",\n",
    "    \"lol\":\"laugh out loud\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text, contractions_dict)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-word characters except spaces\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80d8e457-a279-44d2-a077-590bd3b240a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      dalits are lowlives\n",
       "1                   gay people are a burden to our society\n",
       "2                                    arabs are not welcome\n",
       "3        i am not saying we should actually eliminate h...\n",
       "4                             bananas are for black people\n",
       "                               ...                        \n",
       "41139                       indian culture is so backwards\n",
       "41140    women do not want equality they just want to b...\n",
       "41141                                           fuck covid\n",
       "41142                             this computer is garbage\n",
       "41143                   the only good muslim is a dead one\n",
       "Name: clean_text, Length: 41134, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text'] = data['comment'].apply(clean_text)\n",
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "452bef48-9e80-4954-9b91-12c118cb743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Replace characters\n",
    "    text = text.replace(\"$\", \"s\").replace(\"@\", \"a\").replace(\"3\",\"e\")\n",
    "    # Remove hashtags\n",
    "    text = ' '.join(word for word in text.split() if not word.startswith('#'))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88daa481-d013-4aec-9220-40271759a015",
   "metadata": {},
   "source": [
    "# Handling Recurring Characters and Normalizing Repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7a4689b-b181-4363-9120-ade523c8e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def handle_recurring_characters(text):\n",
    "    # Define the pattern to match consecutive repeated characters (2 or more)\n",
    "    pattern = r'(.)\\1+'\n",
    "    \n",
    "    # Use regex to substitute recurring characters with a single instance\n",
    "    text = re.sub(pattern, r'\\1', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1bff7-b5a4-452f-b6d7-ef8f62cb89c2",
   "metadata": {},
   "source": [
    "# Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4d7ce4a-33fa-4283-a95c-fed3d4eea93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_special_characters(text):\n",
    "    # Define the pattern for special characters using regex\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'  # Matches any character that is not a letter, digit, or whitespace\n",
    "    # Use regex to substitute special characters with an empty string\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6c33b-3982-4c51-b7b0-e8bfe0b4223c",
   "metadata": {},
   "source": [
    "# Removing Numbers and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e70a9be-8789-4e04-881a-4fc8147cceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_noise(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    return text\n",
    "\n",
    "data['comment'] = data['comment'].apply(remove_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b167e5-93eb-479d-bade-0c53987294b6",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8520e-fad9-435d-87c6-9470a4e870f5",
   "metadata": {},
   "source": [
    "### Tokenization is the task of segmenting text into smaller units called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9aa7d9ff-fa4e-46b5-ab47-7e6bb4b2cec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>N</td>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>[dalits, are, lowlives]</td>\n",
       "      <td>[dalits, are, lowlives]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>N</td>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>[gay, people, are, a, burden, to, our, society]</td>\n",
       "      <td>[gay, people, are, a, burden, to, our, society]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabs are not welcome</td>\n",
       "      <td>N</td>\n",
       "      <td>arabs are not welcome</td>\n",
       "      <td>[arabs, are, not, welcome]</td>\n",
       "      <td>[arab, are, not, welcome]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Im not saying we should actually eliminate hee...</td>\n",
       "      <td>N</td>\n",
       "      <td>i am not saying we should actually eliminate h...</td>\n",
       "      <td>[i, am, not, saying, we, should, actually, eli...</td>\n",
       "      <td>[i, am, not, saying, we, should, actually, eli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>N</td>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>[bananas, are, for, black, people]</td>\n",
       "      <td>[banana, are, for, black, people]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment label  \\\n",
       "0                                dalits are lowlives     N   \n",
       "1             gay people are a burden to our society     N   \n",
       "2                              Arabs are not welcome     N   \n",
       "3  Im not saying we should actually eliminate hee...     N   \n",
       "4                       bananas are for black people     N   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0                                dalits are lowlives   \n",
       "1             gay people are a burden to our society   \n",
       "2                              arabs are not welcome   \n",
       "3  i am not saying we should actually eliminate h...   \n",
       "4                       bananas are for black people   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                            [dalits, are, lowlives]   \n",
       "1    [gay, people, are, a, burden, to, our, society]   \n",
       "2                         [arabs, are, not, welcome]   \n",
       "3  [i, am, not, saying, we, should, actually, eli...   \n",
       "4                 [bananas, are, for, black, people]   \n",
       "\n",
       "                                   lemmatized_tokens  \n",
       "0                            [dalits, are, lowlives]  \n",
       "1    [gay, people, are, a, burden, to, our, society]  \n",
       "2                          [arab, are, not, welcome]  \n",
       "3  [i, am, not, saying, we, should, actually, eli...  \n",
       "4                  [banana, are, for, black, people]  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "data['comment'] = data['comment'].astype(str)\n",
    "\n",
    "data['tokens'] = data['clean_text'].apply(tokenize_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f674b3-3880-4958-983f-196d4a2eae86",
   "metadata": {},
   "source": [
    "# StopWord Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92063d5a-6323-4d48-94fe-a48fe3367ac6",
   "metadata": {},
   "source": [
    "### Stop words, which are highly occurring words in the document such as â€˜aâ€™, â€˜anâ€™,â€™theâ€™,â€™isâ€™,â€™wasâ€™,â€™willâ€™,â€™wouldâ€™ etc.They provide no meaningful information, especially if we are building a text classification model. Therefore, we have to remove stopwords from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "20bf4b4c-3dd2-4ad3-89a7-842ef2bd6b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>N</td>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>[dalits, lowlives]</td>\n",
       "      <td>[dalits, lowlives]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>N</td>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>[gay, people, burden, society]</td>\n",
       "      <td>[gay, people, burden, society]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabs are not welcome</td>\n",
       "      <td>N</td>\n",
       "      <td>arabs are not welcome</td>\n",
       "      <td>[arabs, welcome]</td>\n",
       "      <td>[arab, welcome]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Im not saying we should actually eliminate hee...</td>\n",
       "      <td>N</td>\n",
       "      <td>i am not saying we should actually eliminate h...</td>\n",
       "      <td>[saying, actually, eliminate, heebs, wish, nat...</td>\n",
       "      <td>[saying, actually, eliminate, heebs, wish, nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>N</td>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>[bananas, black, people]</td>\n",
       "      <td>[banana, black, people]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment label  \\\n",
       "0                                dalits are lowlives     N   \n",
       "1             gay people are a burden to our society     N   \n",
       "2                              Arabs are not welcome     N   \n",
       "3  Im not saying we should actually eliminate hee...     N   \n",
       "4                       bananas are for black people     N   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0                                dalits are lowlives   \n",
       "1             gay people are a burden to our society   \n",
       "2                              arabs are not welcome   \n",
       "3  i am not saying we should actually eliminate h...   \n",
       "4                       bananas are for black people   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                                 [dalits, lowlives]   \n",
       "1                     [gay, people, burden, society]   \n",
       "2                                   [arabs, welcome]   \n",
       "3  [saying, actually, eliminate, heebs, wish, nat...   \n",
       "4                           [bananas, black, people]   \n",
       "\n",
       "                                   lemmatized_tokens  \n",
       "0                                 [dalits, lowlives]  \n",
       "1                     [gay, people, burden, society]  \n",
       "2                                    [arab, welcome]  \n",
       "3  [saying, actually, eliminate, heebs, wish, nat...  \n",
       "4                            [banana, black, people]  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "data['tokens'] = data['tokens'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63217ba1-444f-43eb-af6a-9f5eb77b1b84",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1bec5-b4bc-4457-ae78-b457bb1cf995",
   "metadata": {},
   "source": [
    "#### Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So, it links words with similar meanings to one word. The practical distinction between stemming and lemmatization is that, where stemming merely removes common suffixes from the end of word tokens, lemmatization ensures the output word is an existing normalized form of the word (for example, lemma) that can be found in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8c56f4c3-bb24-4933-bf5d-9423569ddfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  [dalits, are, lowlives]\n",
       "1          [gay, people, are, a, burden, to, our, society]\n",
       "2                                [arab, are, not, welcome]\n",
       "3        [i, am, not, saying, we, should, actually, eli...\n",
       "4                        [banana, are, for, black, people]\n",
       "                               ...                        \n",
       "41139                 [indian, culture, is, so, backwards]\n",
       "41140    [woman, do, not, want, equality, they, just, w...\n",
       "41141                                        [fuck, covid]\n",
       "41142                        [this, computer, is, garbage]\n",
       "41143          [the, only, good, muslim, is, a, dead, one]\n",
       "Name: lemmatized_tokens, Length: 41134, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "data['lemmatized_tokens'] = data['tokens'].apply(lemmatize_words)\n",
    "data['lemmatized_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b781c5a-718d-4a43-905e-a21254b3a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Preprocessed_cleaned_Final_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd349c2c-7147-457f-8803-76de98535d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
