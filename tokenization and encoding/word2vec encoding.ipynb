{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c95929-751a-4fd1-a6e2-846920d26743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "557bc0bf-f409-4919-bba7-9839b3c9d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'hatspeech dataset.csv'\n",
    "dataset = pd.read_csv(file_path, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2919a94-fbb1-4f85-a12c-fa7a139a4784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daea716c-5cd7-42c1-9135-815bad1d8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocess the Text Data\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abd7ed3-f8b3-4d37-ba61-796ad4f0acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the 'comment' column\n",
    "dataset['tokens'] = dataset['comment'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2e32fad-a11d-4e03-8373-f142dcee2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Encode the Comments using spaCy with caching\n",
    "def encode_comments_spacy(docs):\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        vectors.append(doc.vector)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747bded-05e9-4d22-bcd8-842a7da5fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize comments using spaCy\n",
    "docs = list(nlp.pipe(dataset['comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb8b6375-266b-4ca6-90f4-d38858f67127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode comments in batches\n",
    "vector_cache = {}\n",
    "encoded_vectors = encode_comments_spacy(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3027fa39-bc0c-4e28-b4b4-a873192bb7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign encoded vectors to dataset\n",
    "dataset['vector'] = encoded_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac97399f-270a-4b34-9558-2fb98eebd887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment  \\\n",
      "0                                dalits are lowlives   \n",
      "1             gay people are a burden to our society   \n",
      "2                              Arabs are not welcome   \n",
      "3  I'm not saying we should actually eliminate he...   \n",
      "4                       bananas are for black people   \n",
      "\n",
      "                                              tokens  \\\n",
      "0                            [dalits, are, lowlives]   \n",
      "1    [gay, people, are, a, burden, to, our, society]   \n",
      "2                         [arabs, are, not, welcome]   \n",
      "3  [i, not, saying, we, should, actually, elimina...   \n",
      "4                 [bananas, are, for, black, people]   \n",
      "\n",
      "                                              vector  \n",
      "0  [-3.6505, 0.021166643, 0.48216668, 3.0177667, ...  \n",
      "1  [-1.6461616, 4.3466787, -5.2230763, 0.6333113,...  \n",
      "2  [-3.1942499, 2.0221825, -1.8137375, 2.40047, 1...  \n",
      "3  [-1.0117264, 1.699589, -2.4988325, -2.4018066,...  \n",
      "4  [-4.457378, -0.899094, -4.03162, 2.803622, 2.2...  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows with the new 'tokens' and 'vector' columns\n",
    "print(dataset[['comment', 'tokens', 'vector']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0039257f-bd2f-4bc0-bd7c-99aa7d24e909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved\n"
     ]
    }
   ],
   "source": [
    "dataset.to_csv('word2vec encoded.csv', index=False)\n",
    "print(\"File saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4a2bd-5642-48dc-b586-976c94301ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
