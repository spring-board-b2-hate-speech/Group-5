{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bebb3e-3b9a-44c9-a3ba-b739467c95fe",
   "metadata": {},
   "source": [
    "# import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5552eba-3724-402e-be4a-18fadff825e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62c17a2-b7bf-4799-b643-cb8dea1c8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Preprocessed_Final_dataset.csv',encoding='ISO-8859-1',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f68693f6-bf0b-4034-8665-550dabe70e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41093</th>\n",
       "      <td>Indian culture is so backwards</td>\n",
       "      <td>N</td>\n",
       "      <td>indian culture is so backwards</td>\n",
       "      <td>['indian', 'culture', 'backwards']</td>\n",
       "      <td>indian culture backwards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41094</th>\n",
       "      <td>Women don't want equality, they just want to b...</td>\n",
       "      <td>N</td>\n",
       "      <td>women do not want equality they just want to b...</td>\n",
       "      <td>['woman', 'want', 'equality', 'want', 'charge']</td>\n",
       "      <td>woman want equality want charge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41095</th>\n",
       "      <td>fuck covid</td>\n",
       "      <td>P</td>\n",
       "      <td>fuck covid</td>\n",
       "      <td>['fuck', 'covid']</td>\n",
       "      <td>fuck covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41096</th>\n",
       "      <td>This computer is garbage</td>\n",
       "      <td>P</td>\n",
       "      <td>this computer is garbage</td>\n",
       "      <td>['computer', 'garbage']</td>\n",
       "      <td>computer garbage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41097</th>\n",
       "      <td>The only good Muslim is a dead one</td>\n",
       "      <td>N</td>\n",
       "      <td>the only good muslim is a dead one</td>\n",
       "      <td>['good', 'muslim', 'dead', 'one']</td>\n",
       "      <td>good muslim dead one</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment label  \\\n",
       "41093                     Indian culture is so backwards     N   \n",
       "41094  Women don't want equality, they just want to b...     N   \n",
       "41095                                         fuck covid     P   \n",
       "41096                           This computer is garbage     P   \n",
       "41097                 The only good Muslim is a dead one     N   \n",
       "\n",
       "                                              clean_text  \\\n",
       "41093                     indian culture is so backwards   \n",
       "41094  women do not want equality they just want to b...   \n",
       "41095                                         fuck covid   \n",
       "41096                           this computer is garbage   \n",
       "41097                 the only good muslim is a dead one   \n",
       "\n",
       "                                                tokens  \\\n",
       "41093               ['indian', 'culture', 'backwards']   \n",
       "41094  ['woman', 'want', 'equality', 'want', 'charge']   \n",
       "41095                                ['fuck', 'covid']   \n",
       "41096                          ['computer', 'garbage']   \n",
       "41097                ['good', 'muslim', 'dead', 'one']   \n",
       "\n",
       "                                  text  \n",
       "41093         indian culture backwards  \n",
       "41094  woman want equality want charge  \n",
       "41095                       fuck covid  \n",
       "41096                 computer garbage  \n",
       "41097             good muslim dead one  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "432c627a-2d24-445f-bec6-dfcc6c7b8072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41093                 ['indian', 'culture', 'backwards']\n",
       "41094    ['woman', 'want', 'equality', 'want', 'charge']\n",
       "41095                                  ['fuck', 'covid']\n",
       "41096                            ['computer', 'garbage']\n",
       "41097                  ['good', 'muslim', 'dead', 'one']\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6506a85d-77ab-4c7f-9c17-a4d863936fbc",
   "metadata": {},
   "source": [
    "# Word2vec Embedding\n",
    "### Word2Vec is a neural network-based technique used to generate dense vector representations of words (word embeddings) by analyzing the context in which words appear in a large corpus. It captures semantic relationships between words, allowing similar words to have similar vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df8dbd06-1ddc-4ea0-9fce-ef45aab9e661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tokens  \\\n",
      "0                                 ['dalits', 'lowlives']   \n",
      "1                 ['gay', 'people', 'burden', 'society']   \n",
      "2                                    ['arab', 'welcome']   \n",
      "3      ['say', 'actually', 'eliminate', 'heebs', 'wis...   \n",
      "4                         ['bananas', 'black', 'people']   \n",
      "...                                                  ...   \n",
      "41093                 ['indian', 'culture', 'backwards']   \n",
      "41094    ['woman', 'want', 'equality', 'want', 'charge']   \n",
      "41095                                  ['fuck', 'covid']   \n",
      "41096                            ['computer', 'garbage']   \n",
      "41097                  ['good', 'muslim', 'dead', 'one']   \n",
      "\n",
      "                                                word2vec  \n",
      "0      [-0.2778713961809196, 0.1314378326589411, 0.26...  \n",
      "1      [-0.3402217598631978, 0.028047197204279273, 0....  \n",
      "2      [-0.2702828518262035, 0.10874713839668977, 0.2...  \n",
      "3      [-0.3521344596632274, 0.09958703847355153, 0.2...  \n",
      "4      [-0.2624697783651451, 0.18879137858748435, 0.3...  \n",
      "...                                                  ...  \n",
      "41093  [-0.2738427672644748, 0.17165018716717467, 0.2...  \n",
      "41094  [-0.27233731015486284, 0.15778482360567184, 0....  \n",
      "41095  [-0.23123880957855897, 0.23646986758445993, 0....  \n",
      "41096  [-0.21506520834467982, 0.0924716388563747, 0.2...  \n",
      "41097  [-0.3296565243297003, 0.1480309661252029, 0.20...  \n",
      "\n",
      "[41098 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=data['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get the average Word2Vec vector for a list of tokens\n",
    "def get_average_word2vec(tokens, model):\n",
    "    vector_size = model.wv.vector_size\n",
    "    vec = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vec += model.wv[token]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "# Apply the Word2Vec embedding to the 'tokens' column and create a new column 'embedding'\n",
    "data['word2vec'] = data['tokens'].apply(lambda tokens: get_average_word2vec(tokens, model))\n",
    "\n",
    "# Print the DataFrame with the 'embedding' column\n",
    "print(data[['tokens', 'word2vec']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e1041e9-d366-4fb5-999e-c7ba816e3a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.34022176  0.0280472   0.22755992  0.338593    0.24280386  0.35456144\n",
      " -0.06333581 -0.07450576 -0.39754212  0.64561914 -0.54379885  0.10505109\n",
      " -0.35141737 -0.03588455 -0.824682    0.52561221 -0.02166811  0.63311345\n",
      " -0.02961401 -0.0348969  -0.39919733 -0.12866064  0.11712232 -0.23891418\n",
      "  0.13580278 -0.29576098  0.04176102  0.06649251  0.02023623  0.31140289\n",
      "  0.27302766 -0.04220435  0.63909201  0.26196713 -0.33441275 -0.30604809\n",
      "  0.0068475  -0.24134395  0.14092216 -0.13869322 -0.02181389  0.3157464\n",
      " -0.14411364 -0.28157006  0.0965307  -0.09637904  0.47842981 -0.39011551\n",
      " -0.0430036  -0.2808932  -0.06220733 -0.05262495 -0.08578807 -0.06792285\n",
      " -0.23700072 -0.20065238 -0.24785129 -0.29552251 -0.42779953  0.03286628\n",
      "  0.61237776 -0.27098842 -0.1721415  -0.08117192 -0.11439796  0.89349133\n",
      "  0.42009643 -0.34609759 -0.63537831  0.54390265 -0.38526412  0.00952082\n",
      "  0.12777224 -0.34580731 -0.15579754  0.35573361 -0.06659776  0.1183549\n",
      " -0.10265812  0.12814695  0.67741666 -0.42451472  0.05744446  0.06425117\n",
      " -0.27523584 -0.43817104 -0.11761211  0.35656331 -0.46883246  0.30651287\n",
      "  0.20562892 -0.35774484 -0.07429815 -0.50145073 -0.50828185 -0.67698064\n",
      "  0.24690149  0.51953766  0.52416855  0.5485553 ]\n"
     ]
    }
   ],
   "source": [
    "print(data['word2vec'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d10b2a-cda1-436c-8c79-25fa931cf4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"word2vec_embedding.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c35704d-2e7c-4cd6-8fab-50d46dfd7a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convert 'N' to 0 and 'P' to 1 in the label column\n",
    "data['label'] = label_encoder.fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc2f77-fb9c-42d5-97a2-0f35b95d47ab",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37555df8-fdac-4c01-8779-31104f0ef785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5715328467153284\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.77      0.66      4416\n",
      "           1       0.56      0.34      0.43      3804\n",
      "\n",
      "    accuracy                           0.57      8220\n",
      "   macro avg       0.57      0.56      0.54      8220\n",
      "weighted avg       0.57      0.57      0.55      8220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Extract the features and labels\n",
    "X = np.vstack(data['word2vec'].values)\n",
    "y = data['label'].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logistic_regression_model = LogisticRegression(max_iter=1000) \n",
    "\n",
    "# Train the model\n",
    "logistic_regression_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = logistic_regression_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_str = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4167635-41b6-4e5b-a935-f6ad09ef74b8",
   "metadata": {},
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec443e5c-6b02-4c28-9b83-0179aee2661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45888077858880777\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.00      0.00      4416\n",
      "           1       0.46      0.99      0.63      3804\n",
      "\n",
      "    accuracy                           0.46      8220\n",
      "   macro avg       0.31      0.50      0.32      8220\n",
      "weighted avg       0.30      0.46      0.29      8220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "X = np.vstack(data['word2vec'].values)\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "svm_model = SVC(kernel='linear', max_iter=1000) \n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_str = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da233f-6a3e-4bf6-a7a3-79e6cef57623",
   "metadata": {},
   "source": [
    "### \"After evaluating the performance of different models using TF-IDF and Word2Vec embeddings, we have decided to proceed with TF-IDF embeddings due to its superior performance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ebb41-b1b8-460c-a841-f7ee27881501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
