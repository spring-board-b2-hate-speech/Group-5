{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83cf309-85b6-405f-84c9-bb7ada586539",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4746eb-24b5-49a0-970f-9b2137879529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.5.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\n",
      "   ---------------------------------------- 0.0/258.3 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/258.3 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/258.3 kB 435.7 kB/s eta 0:00:01\n",
      "   ------ -------------------------------- 41.0/258.3 kB 393.8 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 61.4/258.3 kB 409.6 kB/s eta 0:00:01\n",
      "   ------------- ------------------------- 92.2/258.3 kB 435.7 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 122.9/258.3 kB 479.3 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 184.3/258.3 kB 617.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 235.5/258.3 kB 654.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 258.3/258.3 kB 689.1 kB/s eta 0:00:00\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "!pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9886b8de-82d0-4794-94f6-6c3fa9b72f12",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "026ba73f-a863-4607-8a26-16ba0b9caa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd. read_csv('dataset1_utf8.csv',encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf41de97-c4ae-4722-836a-c4d78e7f8fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Arabs are not welcome</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I'm not saying we should actually eliminate he...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41139</th>\n",
       "      <td>117100</td>\n",
       "      <td>Indian culture is so backwards</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41140</th>\n",
       "      <td>118100</td>\n",
       "      <td>Women don't want equality, they just want to b...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41141</th>\n",
       "      <td>119100</td>\n",
       "      <td>fuck covid</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41142</th>\n",
       "      <td>1205</td>\n",
       "      <td>This computer is garbage</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41143</th>\n",
       "      <td>121100</td>\n",
       "      <td>The only good Muslim is a dead one</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41144 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                            comment label\n",
       "0               1                                dalits are lowlives     N\n",
       "1               2             gay people are a burden to our society     N\n",
       "2               3                              Arabs are not welcome     N\n",
       "3               4  I'm not saying we should actually eliminate he...     N\n",
       "4               5                       bananas are for black people     N\n",
       "...           ...                                                ...   ...\n",
       "41139      117100                     Indian culture is so backwards     N\n",
       "41140      118100  Women don't want equality, they just want to b...     N\n",
       "41141      119100                                         fuck covid     P\n",
       "41142        1205                           This computer is garbage     P\n",
       "41143      121100                 The only good Muslim is a dead one     N\n",
       "\n",
       "[41144 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee30f59-d2c0-4898-a5d5-1a2684a4183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "   Unnamed: 0                                            comment label\n",
      "0           1                                dalits are lowlives     N\n",
      "1           2             gay people are a burden to our society     N\n",
      "2           3                              Arabs are not welcome     N\n",
      "3           4  I'm not saying we should actually eliminate he...     N\n",
      "4           5                       bananas are for black people     N\n",
      "\n",
      "Basic information about the dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41144 entries, 0 to 41143\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  41144 non-null  int64 \n",
      " 1   comment     41144 non-null  object\n",
      " 2   label       41144 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 964.4+ KB\n",
      "None\n",
      "\n",
      "Missing values in the dataset:\n",
      "Unnamed: 0    0\n",
      "comment       0\n",
      "label         0\n",
      "dtype: int64\n",
      "\n",
      "Basic statistical details of the dataset:\n",
      "           Unnamed: 0                   comment  label\n",
      "count    41144.000000                     41144  41144\n",
      "unique            NaN                     41134      3\n",
      "top               NaN  Blacks are such pondlife      N\n",
      "freq              NaN                         5  22158\n",
      "mean     20703.991056                       NaN    NaN\n",
      "std      12580.884040                       NaN    NaN\n",
      "min          1.000000                       NaN    NaN\n",
      "25%      10295.750000                       NaN    NaN\n",
      "50%      20593.500000                       NaN    NaN\n",
      "75%      30894.250000                       NaN    NaN\n",
      "max     331000.000000                       NaN    NaN\n",
      "\n",
      "Column names in the dataset:\n",
      "Index(['Unnamed: 0', 'comment', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('dataset1_utf8.csv', encoding='latin1')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nBasic information about the dataset:\")\n",
    "print(data.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in the dataset:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Get basic statistical details\n",
    "print(\"\\nBasic statistical details of the dataset:\")\n",
    "print(data.describe(include='all'))\n",
    "\n",
    "# Display the column names\n",
    "print(\"\\nColumn names in the dataset:\")\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf760554-fcaa-4c03-be8e-9b78ff9778b2",
   "metadata": {},
   "source": [
    "# Distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4210a9-3b9b-4ef6-83f1-0b663bcf6f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "N    22158\n",
      "P    18950\n",
      "O       36\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    False\n",
       "comment       False\n",
       "label         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data['label'].value_counts())  \n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec2de604-6118-48f2-b18d-0bfcc931dd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows in the dataset:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, comment, label]\n",
      "Index: []\n",
      "\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "\n",
    "# Display the duplicate rows\n",
    "print(\"Duplicate rows in the dataset:\")\n",
    "print(duplicate_rows)\n",
    "\n",
    "# Count of duplicate rows\n",
    "num_duplicates = duplicate_rows.shape[0]\n",
    "print(f\"\\nNumber of duplicate rows: {num_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "790e5ea6-9b43-4c50-b2f7-7005b36c3dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the dataset:\n",
      "Index(['Unnamed: 0', 'comment', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('dataset1_utf8.csv', encoding='latin1')\n",
    "\n",
    "# Display the column names\n",
    "print(\"Column names in the dataset:\")\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f17569-a09b-461d-af54-55dd7140ce59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\suman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\suman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b0844-984b-4901-837f-830c241c70c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5396761-f8e5-41ee-bf50-d9b42d5f0ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)# Replace sequences of the same characters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "921208aa-02ef-4f9c-b87d-d8d723445b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['comment'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78c1ce87-c3f6-4623-b188-964a42f78124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      dalits are lowlives\n",
       "1                   gay people are a burden to our society\n",
       "2                                    arabs are not welcome\n",
       "3        i m not saying we should actually eliminate he...\n",
       "4                             bananas are for black people\n",
       "                               ...                        \n",
       "41139                       indian culture is so backwards\n",
       "41140    women don t want equality they just want to be...\n",
       "41141                                           fuck covid\n",
       "41142                             this computer is garbage\n",
       "41143                   the only good muslim is a dead one\n",
       "Name: clean_text, Length: 41144, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3abd9a6-2f13-465d-b0ef-71e9bffad462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a sentence.</td>\n",
       "      <td>[This, is, a, sentence, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Another sentence here.</td>\n",
       "      <td>[Another, sentence, here, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               clean_text                        tokens\n",
       "0     This is a sentence.    [This, is, a, sentence, .]\n",
       "1  Another sentence here.  [Another, sentence, here, .]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Apply word_tokenize to the 'clean_text' column\n",
    "data['tokens'] = data['clean_text'].apply(word_tokenize)\n",
    "\n",
    "(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b410eb-a484-4285-8fc4-725d95bb6539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [This, is, a, sentence, .]\n",
       "1    [Another, sentence, here, .]\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c2606f9-0109-4a13-847b-026d35e4cf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'on', 'ma', 'just', 'no', 't', 'don', 'is', 've', 'so', 'being', 'above', \"isn't\", 'each', 'you', 'we', 'yours', \"haven't\", \"needn't\", 'they', 'ourselves', 'how', 'shan', 's', 'this', 'hasn', 'more', 'he', 'why', 'only', 'when', 'the', 'should', 'there', 'did', 'some', 'whom', 'from', 'our', \"mustn't\", \"couldn't\", 'couldn', \"wasn't\", \"mightn't\", 'them', 'very', 'those', 'shouldn', 'it', 'my', 'again', 'a', \"didn't\", 'because', 'in', 'few', 'own', 'who', 'below', 'other', 'themselves', 'until', 'off', 'what', 'into', 'here', 'by', 'me', \"hasn't\", 'do', 'after', 'than', 'during', 'y', 'yourselves', 'd', 'where', 'itself', 'if', 'between', 'have', 'ain', 'wasn', 'himself', 'as', 'she', 'their', 'your', \"weren't\", 'but', 'will', \"shan't\", 'which', 'any', 'won', \"wouldn't\", \"you'll\", 'does', 'for', 'didn', \"won't\", \"you'd\", 'and', 'll', 'these', 'can', 'm', \"you're\", 'up', 'further', 'both', 'haven', 'theirs', 'were', 'weren', 'under', 'his', 'out', 'yourself', 'her', \"should've\", 'against', 'has', 'before', 'him', 'now', 're', \"you've\", 'i', \"it's\", 'not', 'to', 'was', 'at', 'with', 'that', 'isn', 'mustn', 'or', \"shouldn't\", 'be', 'herself', 'through', 'once', 'same', 'doing', \"she's\", 'had', 'been', 'nor', \"hadn't\", 'doesn', 'having', 'too', 'all', \"doesn't\", 'its', 'needn', 'o', 'ours', 'about', 'are', 'hadn', 'while', 'aren', 'wouldn', 'most', 'am', 'down', 'over', 'an', \"that'll\", 'hers', 'mightn', 'of', \"aren't\", \"don't\", 'such', 'then', 'myself'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords dataset if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(stop_words)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8ae6113-a427-4c99-99b8-7c0f45972ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               clean_text                  tokens\n",
      "0     This is a sentence.           [sentence, .]\n",
      "1  Another sentence here.  [Another, sentence, .]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\suman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample data for demonstration\n",
    "data = pd.DataFrame({'clean_text': ['This is a sentence.', 'Another sentence here.']})\n",
    "\n",
    "# Set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words from a list of tokens\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Tokenize the 'clean_text' column\n",
    "data['tokens'] = data['clean_text'].apply(word_tokenize)\n",
    "\n",
    "# Apply the remove_stopwords function to the 'tokens' column\n",
    "data['tokens'] = data['tokens'].apply(remove_stopwords)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "437a70ef-a801-49f8-9964-68bed6a3f707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             [sentence, .]\n",
      "1    [Another, sentence, .]\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b4a58-02ba-4440-ad89-864eadb295dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "0                                       [dalits, lowlives]\n",
    "1                           [gay, people, burden, society]\n",
    "2                                         [arabs, welcome]\n",
    "3        [saying, actually, eliminate, heebs, wish, nat...\n",
    "4                                 [bananas, black, people]\n",
    "                               ...                        \n",
    "41139                         [indian, culture, backwards]\n",
    "41140                [women, want, equality, want, charge]\n",
    "41141                                        [fuck, covid]\n",
    "41142                                  [computer, garbage]\n",
    "41143                            [good, muslim, dead, one]\n",
    "Name: tokens, Length: 41144, dtype: object\n",
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "757c0858-c1e2-44b9-ae26-be47a93b01de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\suman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\suman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.5.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "   Unnamed: 0                                            comment label\n",
      "0           1                                dalits are lowlives     N\n",
      "1           2             gay people are a burden to our society     N\n",
      "2           3                              Arabs are not welcome     N\n",
      "3           4  I'm not saying we should actually eliminate he...     N\n",
      "4           5                       bananas are for black people     N\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41144 entries, 0 to 41143\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  41144 non-null  int64 \n",
      " 1   comment     41144 non-null  object\n",
      " 2   label       41144 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 964.4+ KB\n",
      "None\n",
      "label\n",
      "N    22158\n",
      "P    18950\n",
      "O       36\n",
      "Name: count, dtype: int64\n",
      "Unnamed: 0    False\n",
      "comment       False\n",
      "label         False\n",
      "dtype: bool\n",
      "0        False\n",
      "1        False\n",
      "2        False\n",
      "3        False\n",
      "4        False\n",
      "         ...  \n",
      "41139    False\n",
      "41140    False\n",
      "41141    False\n",
      "41142    False\n",
      "41143    False\n",
      "Length: 41144, dtype: bool\n",
      "                                             comment  \\\n",
      "0                                dalits are lowlives   \n",
      "1             gay people are a burden to our society   \n",
      "2                              Arabs are not welcome   \n",
      "3  I'm not saying we should actually eliminate he...   \n",
      "4                       bananas are for black people   \n",
      "\n",
      "                                          clean_text  \\\n",
      "0                                dalits are lowlives   \n",
      "1             gay people are a burden to our society   \n",
      "2                              arabs are not welcome   \n",
      "3  i m not saying we should actually eliminate he...   \n",
      "4                       bananas are for black people   \n",
      "\n",
      "                                              tokens  \\\n",
      "0                                 [dalits, lowlives]   \n",
      "1                     [gay, people, burden, society]   \n",
      "2                                    [arab, welcome]   \n",
      "3  [saying, actually, eliminate, heebs, wish, nat...   \n",
      "4                            [banana, black, people]   \n",
      "\n",
      "                                      processed_text  \n",
      "0                                    dalits lowlives  \n",
      "1                          gay people burden society  \n",
      "2                                       arab welcome  \n",
      "3  saying actually eliminate heebs wish naturally...  \n",
      "4                                banana black people  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Install imbalanced-learn for SMOTE\n",
    "!pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('dataset1_utf8.csv', encoding='latin1')  \n",
    "print(data.head())  # Check the first few rows of the dataset\n",
    "print(data.info())\n",
    "print(data['label'].value_counts())\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().any())\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = data.duplicated(subset=['comment'])\n",
    "print(duplicates)\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text) # Replace sequences of the same characters\n",
    "    return text\n",
    "\n",
    "# Apply clean_text function to the comments\n",
    "data['clean_text'] = data['comment'].apply(clean_text)\n",
    "\n",
    "# Tokenize the clean text\n",
    "data['tokens'] = data['clean_text'].apply(word_tokenize)\n",
    "\n",
    "# Set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Apply remove_stopwords function to the tokens\n",
    "data['tokens'] = data['tokens'].apply(remove_stopwords)\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Apply lemmatization\n",
    "data['tokens'] = data['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Join tokens back into a single string for TF-IDF vectorization\n",
    "data['processed_text'] = data['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Display the processed text data\n",
    "print(data[['comment', 'clean_text', 'tokens', 'processed_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2f58c-e551-4bca-bde2-30a267f3b59c",
   "metadata": {},
   "source": [
    "# Here, I'll demonstrate how to implement and compare these tokenization techniques:\n",
    "\n",
    "1. Whitespace Tokenization\n",
    "This is the simplest form of tokenization, where the text is split by whitespace.\n",
    "\n",
    "2. Word Tokenization using NLTK\n",
    "This uses the word_tokenize function from the NLTK library.\n",
    "\n",
    "3. Regular Expression Tokenization\n",
    "This uses regular expressions to split the text based on patterns.\n",
    "\n",
    "4. SpaCy Tokenization\n",
    "This uses the SpaCy library, which provides advanced tokenization techniques.\n",
    "\n",
    "Here's the complete code to compare these techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1812f-655c-4b8c-b6d0-3d77250795e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\suman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\suman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.2.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.7.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (70.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 640.0 kB/s eta 0:00:20\n",
      "     --------------------------------------- 0.0/12.8 MB 262.6 kB/s eta 0:00:49\n",
      "     --------------------------------------- 0.1/12.8 MB 508.4 kB/s eta 0:00:26\n",
      "      -------------------------------------- 0.2/12.8 MB 871.5 kB/s eta 0:00:15\n",
      "      --------------------------------------- 0.3/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     - -------------------------------------- 0.4/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.5 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.5 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 0.6/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 0.6/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.6/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.6/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     -- ------------------------------------- 0.7/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     -- ------------------------------------- 0.9/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     --- ------------------------------------ 1.1/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     --- ------------------------------------ 1.2/12.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.7/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.9/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 1.9/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 2.0/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 2.0/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 2.2/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.3/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.3/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.5/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.5/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.5/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.5/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.8/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 2.9/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 3.0/12.8 MB 1.6 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.6 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 3.2/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 3.9/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 4.0/12.8 MB 1.9 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.2/12.8 MB 1.9 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.4/12.8 MB 1.9 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.5/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.7/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.8/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 4.9/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.1/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 5.4/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.8/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.8/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.9/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.9/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.9/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.9/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.9/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.2/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.3/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.5/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.5/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.6/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.7/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 6.8/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 6.9/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 7.0/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.4/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.7/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.9/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 8.0/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 8.0/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 8.0/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 8.0/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 8.0/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.0/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.0/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.0/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.0/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.0/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.0/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.0/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.2/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.2/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.3/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.3/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.4/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.4/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.5/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.6/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.7/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.8/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.9/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.9/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 9.0/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 9.3/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 1.7 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.6/12.8 MB 1.6 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 1.7 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.0/12.8 MB 1.7 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.1/12.8 MB 1.7 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.2/12.8 MB 1.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.3/12.8 MB 1.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.3/12.8 MB 1.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.4/12.8 MB 1.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.5/12.8 MB 1.7 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.6/12.8 MB 1.7 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.7/12.8 MB 1.7 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.8/12.8 MB 1.7 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.9/12.8 MB 1.8 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 1.8 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 1.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 1.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 1.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 1.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 1.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 1.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.5/12.8 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (70.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\suman\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "   Unnamed: 0                                            comment label\n",
      "0           1                                dalits are lowlives     N\n",
      "1           2             gay people are a burden to our society     N\n",
      "2           3                              Arabs are not welcome     N\n",
      "3           4  I'm not saying we should actually eliminate he...     N\n",
      "4           5                       bananas are for black people     N\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41144 entries, 0 to 41143\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  41144 non-null  int64 \n",
      " 1   comment     41144 non-null  object\n",
      " 2   label       41144 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 964.4+ KB\n",
      "None\n",
      "label\n",
      "N    22158\n",
      "P    18950\n",
      "O       36\n",
      "Name: count, dtype: int64\n",
      "Unnamed: 0    False\n",
      "comment       False\n",
      "label         False\n",
      "dtype: bool\n",
      "0        False\n",
      "1        False\n",
      "2        False\n",
      "3        False\n",
      "4        False\n",
      "         ...  \n",
      "41139    False\n",
      "41140    False\n",
      "41141    False\n",
      "41142    False\n",
      "41143    False\n",
      "Length: 41144, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Install necessary libraries\n",
    "!pip install spacy\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load SpaCy and its language model\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('dataset1_utf8.csv', encoding='latin1')  \n",
    "print(data.head())  # Check the first few rows of the dataset\n",
    "print(data.info())\n",
    "print(data['label'].value_counts())\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().any())\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = data.duplicated(subset=['comment'])\n",
    "print(duplicates)\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text) # Replace sequences of the same characters\n",
    "    return text\n",
    "\n",
    "# Apply clean_text function to the comments\n",
    "data['clean_text'] = data['comment'].apply(clean_text)\n",
    "\n",
    "# Tokenization techniques\n",
    "\n",
    "# 1. Whitespace Tokenization\n",
    "data['whitespace_tokens'] = data['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# 2. NLTK Word Tokenization\n",
    "data['nltk_tokens'] = data['clean_text'].apply(word_tokenize)\n",
    "\n",
    "# 3. Regular Expression Tokenization\n",
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['regexp_tokens'] = data['clean_text'].apply(regexp_tokenizer.tokenize)\n",
    "\n",
    "# 4. SpaCy Tokenization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "data['spacy_tokens'] = data['clean_text'].apply(lambda x: [token.text for token in nlp(x)])\n",
    "\n",
    "# Set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Function to lemmatize tokens\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Apply preprocessing steps to each tokenization method\n",
    "for method in ['whitespace_tokens', 'nltk_tokens', 'regexp_tokens', 'spacy_tokens']:\n",
    "    data[method] = data[method].apply(remove_stopwords)\n",
    "    data[method] = data[method].apply(lemmatize_tokens)\n",
    "\n",
    "# Show the processed text for each tokenization technique\n",
    "print(data[['comment', 'clean_text', 'whitespace_tokens', 'nltk_tokens', 'regexp_tokens', 'spacy_tokens']].head())\n",
    "\n",
    "# Example of selecting the best tokenization method\n",
    "# Here, we could use various metrics such as the quality of features for a model, but for simplicity, we'll consider the token count.\n",
    "\n",
    "# Compare token counts\n",
    "data['whitespace_token_count'] = data['whitespace_tokens'].apply(len)\n",
    "data['nltk_token_count'] = data['nltk_tokens'].apply(len)\n",
    "data['regexp_token_count'] = data['regexp_tokens'].apply(len)\n",
    "data['spacy_token_count'] = data['spacy_tokens'].apply(len)\n",
    "\n",
    "print(data[['whitespace_token_count', 'nltk_token_count', 'regexp_token_count', 'spacy_token_count']].describe())\n",
    "\n",
    "# Assuming we select the tokenization method with the most balanced token count as the best one\n",
    "# For demonstration, let's assume NLTK Word Tokenization is the best one\n",
    "\n",
    "data['best_tokens'] = data['nltk_tokens']\n",
    "\n",
    "# Join tokens back into a single string for TF-IDF vectorization\n",
    "data['processed_text'] = data['best_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Display the final processed text data\n",
    "print(data[['comment', 'clean_text', 'processed_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737714c-3438-4938-8b5a-87939d62f93b",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b9e5f4-5e4f-4348-a940-966f270bfd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Whitespace Tokenization: Splits text based on whitespace.\n",
    "NLTK Word Tokenization: Uses word_tokenize from NLTK.\n",
    "Regular Expression Tokenization: Uses RegexpTokenizer from NLTK.\n",
    "SpaCy Tokenization: Uses SpaCyâ€™s tokenizer.\n",
    "After tokenizing the text using each method, stop words are removed, and tokens are lemmatized. Finally, the token count for each method is compared to evaluate the balance and consistency of tokens produced.\n",
    "\n",
    "Evaluation:\n",
    "For demonstration, we can print and compare the token counts. In practice, you might choose the best method based on the quality of features for a machine learning model or other specific criteria relevant to your analysis. For simplicity, the NLTK Word Tokenization is selected as the best method in this example.\n",
    "\n",
    "Adjust the selection criteria according to your specific requirements and analysis goals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
