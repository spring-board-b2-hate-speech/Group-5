{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa08305-427c-4a72-a1f9-684de1ff8217",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d565d-d5ab-4465-8a7e-179608bfe81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a9f1a-464f-4ffb-b18d-d866531756b1",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee8bad-7dfa-4cd4-b17c-a98de490d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('hatspeech dataset.csv', encoding='latin1')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81ac81-b73d-4607-bec9-a1dbcb9f3940",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680121b-5cc4-4a97-b89d-55f0c7222bc5",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bc39d-7201-408f-a4df-127c2bfc10a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())  # Check the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d0e08-8ecb-4fa1-bc2c-6d628573d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())  # Get information about the dataset, like column names and data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99633d91-ff78-4a2f-ac18-95265e7fde96",
   "metadata": {},
   "source": [
    "# Checking the distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78c3d1-40c8-4944-ba17-17c23e32545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['label'].value_counts())  # Check the distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dfdd9-96b7-4521-819a-e00f2b999dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526711de-38c8-4a07-a845-9befc0201b61",
   "metadata": {},
   "source": [
    "# Checking for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a0729a-1940-4b1e-8a6d-d0a5bb459d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = data.duplicated(subset=['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37552e65-320d-4d73-96b6-a6102d485658",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294a7c1-5e46-459c-a8f7-4586788b01b2",
   "metadata": {},
   "source": [
    "# Importing necessary nltk packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8626d76d-3ecf-499a-ac2e-8e9f04e4fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ca472-3c80-4a41-ac43-297f7057f43c",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957c504-4ed0-48f3-8bbe-c60354e8d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6724ef4-cd64-43f2-b069-16eb299ce77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['comment'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507fee2-826d-4a3b-8cdd-b28c734e80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a0295-7b57-46b8-a2f1-2a3495c3a31e",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86b099-09c4-47ae-98ee-bc0f44f0f932",
   "metadata": {},
   "source": [
    "Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords.For example, tokenizing the sentence “I love ice cream” would result in three tokens: “I,” “love,” and “ice cream.” It’s a fundamental step in natural language processing and text analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc23ea-a75e-47fa-a504-fd822f191bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data['tokens'] = data['clean_text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a08ff-1e93-4cb0-a4fb-9d59d5c4fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a01384-8bd0-434e-ba4d-95cff394b365",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42740df-c988-4bcc-8419-91308019b97f",
   "metadata": {},
   "source": [
    "Stop words, which are highly occurring words in the document such as ‘a’, ‘an’,’the’,’is’,’was’,’will’,’would’ etc.They provide no meaningful information, especially if we are building a text classification model. Therefore, we have to remove stopwords from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e32689-ca16-42fd-b9ea-608507ec83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9e34c-d542-4485-b90e-cefaf49438e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a6702-bfa2-4ff2-97fa-e2752f74ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['tokens'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb10b57-30c3-44a1-98f3-2c997a2ccdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993ee70-4932-4842-b112-ed663882627a",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d59e41-3a1a-42ea-90b5-38a80538e130",
   "metadata": {},
   "source": [
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So, it links words with similar meanings to one word. The practical distinction between stemming and lemmatization is that, where stemming merely removes common suffixes from the end of word tokens, lemmatization ensures the output word is an existing normalized form of the word (for example, lemma) that can be found in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455e32d-2289-4a91-b443-9f48b85c5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "data['lemmatized_tokens'] = data['tokens'].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298bc0c-6239-42a9-b921-9c8255d37a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemmatized_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a9374-7f55-435c-aeef-549c23a0c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('hatespeech_Preprocessed.csv', index=False)\n",
    "print(\"File saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f12025-4f2b-430e-8024-c8f5e1454129",
   "metadata": {},
   "source": [
    "# Vectorization (using TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9a31c-f2dc-441e-a252-cb1798b88a0d",
   "metadata": {},
   "source": [
    "TF-IDF is the importance of a term is inversely related to its frequency across documents.TF gives us information on how often a term appears in a document and IDF gives us information about the relative rarity of a term in the collection of documents. By multiplying these values together we can get our final TF-IDF value.The higher the TF-IDF score the more important or relevant the term is; as a term gets less relevant, its TF-IDF score will approach 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b1917-e270-4b82-ba88-9c0bd06bbab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf_vectorizer.fit_transform(data['clean_text']).toarray()\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01627f5-6edd-4c28-846c-1a409da484a5",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9300f-a023-4e5c-b5a7-03b0a5136d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8102883-8322-483b-b341-d4e04f0ba200",
   "metadata": {},
   "source": [
    "# Class Balancing using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acc429-2b48-4830-a053-40867984ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867dcd8-e73e-4527-9274-c39c798517ba",
   "metadata": {},
   "source": [
    "# Checking the class distribution after balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86a33e-0c10-4762-9c7a-9926e007ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before SMOTE:\", y_train.value_counts())\n",
    "print(\"After SMOTE:\", y_train_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e184a8-6127-4d0b-ae9b-dd1c324b2e64",
   "metadata": {},
   "source": [
    "# Saving the balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a2d46-e966-48b8-b1f9-1e5a07672bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Save the balanced data\n",
    "balanced_data = pd.DataFrame(X_train_balanced, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "balanced_data['label'] = y_train_balanced\n",
    "balanced_data.to_csv('balanced_hate_comment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197cba4-c83c-452d-80f3-93e7b6eb6006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
