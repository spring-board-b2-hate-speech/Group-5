{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa08305-427c-4a72-a1f9-684de1ff8217",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "e93d565d-d5ab-4465-8a7e-179608bfe81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a9f1a-464f-4ffb-b18d-d866531756b1",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "16ee8bad-7dfa-4cd4-b17c-a98de490d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('hatspeech dataset.csv', encoding='latin1')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "5a81ac81-b73d-4607-bec9-a1dbcb9f3940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>dalits are lowlives</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>gay people are a burden to our society</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Arabs are not welcome</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I'm not saying we should actually eliminate he...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>bananas are for black people</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41139</th>\n",
       "      <td>117100</td>\n",
       "      <td>Indian culture is so backwards</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41140</th>\n",
       "      <td>118100</td>\n",
       "      <td>Women don't want equality, they just want to b...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41141</th>\n",
       "      <td>119100</td>\n",
       "      <td>fuck covid</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41142</th>\n",
       "      <td>1205</td>\n",
       "      <td>This computer is garbage</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41143</th>\n",
       "      <td>121100</td>\n",
       "      <td>The only good Muslim is a dead one</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41144 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                            comment label\n",
       "0               1                                dalits are lowlives     N\n",
       "1               2             gay people are a burden to our society     N\n",
       "2               3                              Arabs are not welcome     N\n",
       "3               4  I'm not saying we should actually eliminate he...     N\n",
       "4               5                       bananas are for black people     N\n",
       "...           ...                                                ...   ...\n",
       "41139      117100                     Indian culture is so backwards     N\n",
       "41140      118100  Women don't want equality, they just want to b...     N\n",
       "41141      119100                                         fuck covid     P\n",
       "41142        1205                           This computer is garbage     P\n",
       "41143      121100                 The only good Muslim is a dead one     N\n",
       "\n",
       "[41144 rows x 3 columns]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680121b-5cc4-4a97-b89d-55f0c7222bc5",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "100bc39d-7201-408f-a4df-127c2bfc10a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                            comment label\n",
      "0           1                                dalits are lowlives     N\n",
      "1           2             gay people are a burden to our society     N\n",
      "2           3                              Arabs are not welcome     N\n",
      "3           4  I'm not saying we should actually eliminate he...     N\n",
      "4           5                       bananas are for black people     N\n"
     ]
    }
   ],
   "source": [
    "print(data.head())  # Check the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "a06d0e08-8ecb-4fa1-bc2c-6d628573d8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41144 entries, 0 to 41143\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  41144 non-null  int64 \n",
      " 1   comment     41144 non-null  object\n",
      " 2   label       41144 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 964.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())  # Get information about the dataset, like column names and data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99633d91-ff78-4a2f-ac18-95265e7fde96",
   "metadata": {},
   "source": [
    "# Checking the distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "8a78c3d1-40c8-4944-ba17-17c23e32545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "N    22158\n",
      "P    18950\n",
      "O       36\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['label'].value_counts())  # Check the distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "654dfdd9-96b7-4521-819a-e00f2b999dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    False\n",
       "comment       False\n",
       "label         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526711de-38c8-4a07-a845-9befc0201b61",
   "metadata": {},
   "source": [
    "# Checking for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "08a0729a-1940-4b1e-8a6d-d0a5bb459d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = data.duplicated(subset=['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "37552e65-320d-4d73-96b6-a6102d485658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "41139    False\n",
       "41140    False\n",
       "41141    False\n",
       "41142    False\n",
       "41143    False\n",
       "Length: 41144, dtype: bool"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294a7c1-5e46-459c-a8f7-4586788b01b2",
   "metadata": {},
   "source": [
    "# Importing necessary nltk packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "8626d76d-3ecf-499a-ac2e-8e9f04e4fe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maddi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maddi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maddi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ca472-3c80-4a41-ac43-297f7057f43c",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "1957c504-4ed0-48f3-8bbe-c60354e8d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "d6724ef4-cd64-43f2-b069-16eb299ce77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['comment'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "4507fee2-826d-4a3b-8cdd-b28c734e80fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      dalits are lowlives\n",
       "1                   gay people are a burden to our society\n",
       "2                                    arabs are not welcome\n",
       "3        i m not saying we should actually eliminate he...\n",
       "4                             bananas are for black people\n",
       "                               ...                        \n",
       "41139                       indian culture is so backwards\n",
       "41140    women don t want equality they just want to be...\n",
       "41141                                           fuck covid\n",
       "41142                             this computer is garbage\n",
       "41143                   the only good muslim is a dead one\n",
       "Name: clean_text, Length: 41144, dtype: object"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a0295-7b57-46b8-a2f1-2a3495c3a31e",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86b099-09c4-47ae-98ee-bc0f44f0f932",
   "metadata": {},
   "source": [
    "Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords.For example, tokenizing the sentence “I love ice cream” would result in three tokens: “I,” “love,” and “ice cream.” It’s a fundamental step in natural language processing and text analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "dddc23ea-a75e-47fa-a504-fd822f191bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data['tokens'] = data['clean_text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "566a08ff-1e93-4cb0-a4fb-9d59d5c4fa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  [dalits, are, lowlives]\n",
       "1          [gay, people, are, a, burden, to, our, society]\n",
       "2                               [arabs, are, not, welcome]\n",
       "3        [i, m, not, saying, we, should, actually, elim...\n",
       "4                       [bananas, are, for, black, people]\n",
       "                               ...                        \n",
       "41139                 [indian, culture, is, so, backwards]\n",
       "41140    [women, don, t, want, equality, they, just, wa...\n",
       "41141                                        [fuck, covid]\n",
       "41142                        [this, computer, is, garbage]\n",
       "41143          [the, only, good, muslim, is, a, dead, one]\n",
       "Name: tokens, Length: 41144, dtype: object"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a01384-8bd0-434e-ba4d-95cff394b365",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42740df-c988-4bcc-8419-91308019b97f",
   "metadata": {},
   "source": [
    "Stop words, which are highly occurring words in the document such as ‘a’, ‘an’,’the’,’is’,’was’,’will’,’would’ etc.They provide no meaningful information, especially if we are building a text classification model. Therefore, we have to remove stopwords from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "19e32689-ca16-42fd-b9ea-608507ec83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "37c9e34c-d542-4485-b90e-cefaf49438e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "e30a6702-bfa2-4ff2-97fa-e2752f74ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['tokens'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "5fb10b57-30c3-44a1-98f3-2c997a2ccdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                       [dalits, lowlives]\n",
      "1                           [gay, people, burden, society]\n",
      "2                                         [arabs, welcome]\n",
      "3        [saying, actually, eliminate, heebs, wish, nat...\n",
      "4                                 [bananas, black, people]\n",
      "                               ...                        \n",
      "41139                         [indian, culture, backwards]\n",
      "41140                [women, want, equality, want, charge]\n",
      "41141                                        [fuck, covid]\n",
      "41142                                  [computer, garbage]\n",
      "41143                            [good, muslim, dead, one]\n",
      "Name: tokens, Length: 41144, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993ee70-4932-4842-b112-ed663882627a",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d59e41-3a1a-42ea-90b5-38a80538e130",
   "metadata": {},
   "source": [
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So, it links words with similar meanings to one word. The practical distinction between stemming and lemmatization is that, where stemming merely removes common suffixes from the end of word tokens, lemmatization ensures the output word is an existing normalized form of the word (for example, lemma) that can be found in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "d455e32d-2289-4a91-b443-9f48b85c5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "data['lemmatized_tokens'] = data['tokens'].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "b298bc0c-6239-42a9-b921-9c8255d37a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       [dalits, lowlives]\n",
       "1                           [gay, people, burden, society]\n",
       "2                                          [arab, welcome]\n",
       "3        [saying, actually, eliminate, heebs, wish, nat...\n",
       "4                                  [banana, black, people]\n",
       "                               ...                        \n",
       "41139                         [indian, culture, backwards]\n",
       "41140                [woman, want, equality, want, charge]\n",
       "41141                                        [fuck, covid]\n",
       "41142                                  [computer, garbage]\n",
       "41143                            [good, muslim, dead, one]\n",
       "Name: lemmatized_tokens, Length: 41144, dtype: object"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemmatized_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "562a9374-7f55-435c-aeef-549c23a0c3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved\n"
     ]
    }
   ],
   "source": [
    "data.to_csv('hatespeech_Preprocessed.csv', index=False)\n",
    "print(\"File saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f12025-4f2b-430e-8024-c8f5e1454129",
   "metadata": {},
   "source": [
    "# Vectorization (using TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9a31c-f2dc-441e-a252-cb1798b88a0d",
   "metadata": {},
   "source": [
    "TF-IDF is the importance of a term is inversely related to its frequency across documents.TF gives us information on how often a term appears in a document and IDF gives us information about the relative rarity of a term in the collection of documents. By multiplying these values together we can get our final TF-IDF value.The higher the TF-IDF score the more important or relevant the term is; as a term gets less relevant, its TF-IDF score will approach 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "f07b1917-e270-4b82-ba88-9c0bd06bbab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf_vectorizer.fit_transform(data['clean_text']).toarray()\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01627f5-6edd-4c28-846c-1a409da484a5",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "8cb9300f-a023-4e5c-b5a7-03b0a5136d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8102883-8322-483b-b341-d4e04f0ba200",
   "metadata": {},
   "source": [
    "# Class Balancing using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "49acc429-2b48-4830-a053-40867984ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867dcd8-e73e-4527-9274-c39c798517ba",
   "metadata": {},
   "source": [
    "# Checking the class distribution after balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "fa86a33e-0c10-4762-9c7a-9926e007ac39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: label\n",
      "N    17783\n",
      "P    15100\n",
      "O       32\n",
      "Name: count, dtype: int64\n",
      "After SMOTE: label\n",
      "P    17783\n",
      "N    17783\n",
      "O    17783\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Before SMOTE:\", y_train.value_counts())\n",
    "print(\"After SMOTE:\", y_train_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e184a8-6127-4d0b-ae9b-dd1c324b2e64",
   "metadata": {},
   "source": [
    "# Saving the balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "009a2d46-e966-48b8-b1f9-1e5a07672bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Save the balanced data\n",
    "balanced_data = pd.DataFrame(X_train_balanced, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "balanced_data['label'] = y_train_balanced\n",
    "balanced_data.to_csv('balanced_hate_comment.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
