{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0dd10b4-c761-4e56-acd6-1c913a1fcf02",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6323e78b-7424-44c2-81d4-c5d2528c2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5736ec6-0dae-408b-b299-c6da9473a6cf",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d17cb8-f7e5-4bae-999e-5d1fffebc183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('hatspeech dataset.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127f5ebe-746f-4336-b3df-f82bae54cddf",
   "metadata": {},
   "source": [
    "# Basic data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc071552-90f6-4f08-a10d-2d747ba5c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                            comment label\n",
      "0           1                                dalits are lowlives     N\n",
      "1           2             gay people are a burden to our society     N\n",
      "2           3                              Arabs are not welcome     N\n",
      "3           4  I'm not saying we should actually eliminate he...     N\n",
      "4           5                       bananas are for black people     N\n"
     ]
    }
   ],
   "source": [
    "# Basic data exploration\n",
    "print(data.head())  # Check the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8949dd6f-11ac-4108-8eaa-5a6c79bbb7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41144 entries, 0 to 41143\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  41144 non-null  int64 \n",
      " 1   comment     41144 non-null  object\n",
      " 2   label       41144 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 964.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())  # Get information about the dataset, like column names and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12299936-0dee-4738-8079-22ae26d0b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "N    22158\n",
      "P    18950\n",
      "O       36\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['label'].value_counts())  # Check the distribution of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ea2e1f-3b98-4e7f-a93c-c4e963e59785",
   "metadata": {},
   "source": [
    "# Filter out rows with label 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26971093-b1e4-460f-aa8f-c1cfa3ceb079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "N    22158\n",
      "P    18950\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with label 'O'\n",
    "data = data[data['label'] != 'O'].copy()\n",
    "print(data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc7b88-d51c-4a29-957d-022dd0c649be",
   "metadata": {},
   "source": [
    "# Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ff6c24-794d-4b4f-a34a-db10ae20aff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0    False\n",
      "comment       False\n",
      "label         False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(data.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfd2c0-48aa-465a-a6a2-bb710dd6b0fe",
   "metadata": {},
   "source": [
    "# Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d9cc14c-5642-45ff-bd41-345275047751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "num_duplicates = data.duplicated().sum()\n",
    "print(f'Number of duplicate rows: {num_duplicates}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97719f2-b388-45c6-97a6-1a66963c6a72",
   "metadata": {},
   "source": [
    "# Download necessary NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6c53731-686d-4e41-a0ea-18038466615d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maddi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maddi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maddi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eea4e9-d5fa-4c95-985e-ef7d75e858b1",
   "metadata": {},
   "source": [
    "# Define chatword and slang dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c4fceb-93ae-4e6a-8280-aaae9007ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chatword and slang dictionaries\n",
    "chatword_dictionary = {\n",
    "    'u': 'you',\n",
    "    'ur': 'your',\n",
    "    'r': 'are',\n",
    "    'y': 'why',\n",
    "    'b4': 'before',\n",
    "    'gr8': 'great',\n",
    "    'l8r': 'later',\n",
    "    'w8': 'wait',\n",
    "    'bff': 'best friend forever',\n",
    "    'brb': 'be right back',\n",
    "    'btw': 'by the way',\n",
    "    'cuz': 'because',\n",
    "    'idk': 'i do not know',\n",
    "    'ikr': 'i know right',\n",
    "    'imo': 'in my opinion',\n",
    "    'lmao': 'laughing my ass off',\n",
    "    'lol': 'laugh out loud',\n",
    "    'omg': 'oh my god',\n",
    "    'omw': 'on my way',\n",
    "    'pls': 'please',\n",
    "    'thx': 'thanks',\n",
    "    'ttyl': 'talk to you later',\n",
    "    'wth': 'what the hell',\n",
    "    'wyd': 'what you doing',\n",
    "    'smh': 'shaking my head'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e850d20c-45e1-4bd0-a231-72a11970ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {\n",
    "    \"kinda\": \"kind of\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"idk\": \"I do not know\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"bbl\": \"be back later\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d4d24-084a-4e7a-bf1b-d5a26571c4df",
   "metadata": {},
   "source": [
    "# Define text cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3934509-02d6-4fd4-8797-3c428f1c2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    for chatword, full_form in chatword_dictionary.items():\n",
    "        text = re.sub(rf\"\\b{chatword}\\b\", full_form, text)\n",
    "    for slang, expanded in slang_dict.items():\n",
    "        text = re.sub(r'\\b' + re.escape(slang) + r'\\b', expanded, text)\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)  # Replace sequences of the same character > 2 with 1 occurrence\n",
    "    text = re.sub(r'(\\w)[^\\w\\s](\\w)', r'\\1\\2', text)  # Remove special characters within words like 'b@ckward' to 'backward'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5b1140a-afbf-4d01-a378-5e0488dba7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['comment'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0042df8-6e80-43d1-ace5-052f8fae6223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      dalits are lowlives\n",
       "1                   gay people are a burden to our society\n",
       "2                                    arabs are not welcome\n",
       "3        i am not saying we should actually eliminate h...\n",
       "4                             bananas are for black people\n",
       "                               ...                        \n",
       "41139                       indian culture is so backwards\n",
       "41140    women do not want equality they just want to b...\n",
       "41141                                           fuck covid\n",
       "41142                             this computer is garbage\n",
       "41143                   the only good muslim is a dead one\n",
       "Name: clean_text, Length: 41108, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf33b0f-233c-4b42-a2aa-bc74a39fb471",
   "metadata": {},
   "source": [
    "# Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87538556-5a34-4436-a48e-94f2016b0b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "data['tokens'] = data['clean_text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a70674db-8027-403e-8dbd-24ed6b5416de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  [dalits, are, lowlives]\n",
       "1          [gay, people, are, a, burden, to, our, society]\n",
       "2                               [arabs, are, not, welcome]\n",
       "3        [i, am, not, saying, we, should, actually, eli...\n",
       "4                       [bananas, are, for, black, people]\n",
       "                               ...                        \n",
       "41139                 [indian, culture, is, so, backwards]\n",
       "41140    [women, do, not, want, equality, they, just, w...\n",
       "41141                                        [fuck, covid]\n",
       "41142                        [this, computer, is, garbage]\n",
       "41143          [the, only, good, muslim, is, a, dead, one]\n",
       "Name: tokens, Length: 41108, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e4101-3542-4c92-af73-711a8c44dc84",
   "metadata": {},
   "source": [
    "# Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6786110-95a9-4c0d-94e0-cb6f0717f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stop_words(tokens):\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "data['remove_stopwords'] = data['tokens'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9fc383a-49f7-4ca3-ba5a-333bd43c901c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       [dalits, lowlives]\n",
       "1                           [gay, people, burden, society]\n",
       "2                                         [arabs, welcome]\n",
       "3        [saying, actually, eliminate, heebs, wish, nat...\n",
       "4                                 [bananas, black, people]\n",
       "                               ...                        \n",
       "41139                         [indian, culture, backwards]\n",
       "41140                [women, want, equality, want, charge]\n",
       "41141                                        [fuck, covid]\n",
       "41142                                  [computer, garbage]\n",
       "41143                            [good, muslim, dead, one]\n",
       "Name: remove_stopwords, Length: 41108, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['remove_stopwords']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16abe7-4fe7-4620-b32a-2def2ece513a",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "975aac10-d933-413c-ba27-7f1db54f06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "def perform_stemming(tokens):\n",
    "    return [ps.stem(word) for word in tokens]\n",
    "data['stemmed'] = data['remove_stopwords'].apply(perform_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb1a3df-6bfe-4f12-8b6d-34f71e92dcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          [dalit, lowliv]\n",
       "1                            [gay, peopl, burden, societi]\n",
       "2                                           [arab, welcom]\n",
       "3        [say, actual, elimin, heeb, wish, natur, becam...\n",
       "4                                   [banana, black, peopl]\n",
       "                               ...                        \n",
       "41139                           [indian, cultur, backward]\n",
       "41140                    [women, want, equal, want, charg]\n",
       "41141                                        [fuck, covid]\n",
       "41142                                     [comput, garbag]\n",
       "41143                            [good, muslim, dead, one]\n",
       "Name: stemmed, Length: 41108, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['stemmed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e79e9-4acc-4c7c-8b69-b3c2dd125f94",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ec93dad-aa65-4f64-be04-1a71575cbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def perform_lemmatization(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "data['lemmatized'] = data['remove_stopwords'].apply(perform_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12342e55-dad3-44f7-bef2-42df1fb1dde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       [dalits, lowlives]\n",
       "1                           [gay, people, burden, society]\n",
       "2                                          [arab, welcome]\n",
       "3        [saying, actually, eliminate, heebs, wish, nat...\n",
       "4                                  [banana, black, people]\n",
       "                               ...                        \n",
       "41139                         [indian, culture, backwards]\n",
       "41140                [woman, want, equality, want, charge]\n",
       "41141                                        [fuck, covid]\n",
       "41142                                  [computer, garbage]\n",
       "41143                            [good, muslim, dead, one]\n",
       "Name: lemmatized, Length: 41108, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemmatized']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212c96b-20c8-4879-b922-bfe76237c96e",
   "metadata": {},
   "source": [
    "# Perform TF-IDF embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02c92b9b-b208-406c-96b5-e80635c4645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform TF-IDF embedding\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['lemmatized'].apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51128f5-f339-4306-9feb-89e6721c7d76",
   "metadata": {},
   "source": [
    "# Initialize RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c1afc64-03aa-490b-8acf-21b85038a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495ad17-566f-4b47-8683-ffbe0ed0025c",
   "metadata": {},
   "source": [
    "# Resample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11560154-ce5b-4dca-8786-cdaec5e503e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data\n",
    "X_resampled, y_resampled = ros.fit_resample(X_tfidf, data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1eaac-d40f-4ebd-b163-96b33f9792dd",
   "metadata": {},
   "source": [
    "# Convert back to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb5325e0-2e0f-421a-83bd-7848090f3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to DataFrame if necessary\n",
    "balanced_data = pd.DataFrame(X_resampled.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "balanced_data['label'] = y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd373d76-3fdd-4673-9ef6-6c4a37b40480",
   "metadata": {},
   "source": [
    "# Save balanced data to a new CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a31e288-ea36-4da9-9201-4b53423ddbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save balanced data to a new CSV file\n",
    "balanced_data.to_csv('tfidf balanced_hatespeech_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
